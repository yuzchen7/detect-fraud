{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f485cae",
   "metadata": {},
   "source": [
    "# Data Transform\n",
    "\n",
    "In this notebook, we will ask you a series of questions to evaluate your findings from your EDA. Based on your response & justification, we will ask you to also apply a subsequent data transformation. \n",
    "\n",
    "If you state that you will not apply any data transformations for this step, you must **justify** as to why your dataset/machine-learning does not require the mentioned data preprocessing step.\n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient feature engineering step in this project we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "You will write out this transformed dataframe as a `.csv` file to your `data/` folder.\n",
    "\n",
    "**Note**: Again, note that this dataset is quite large. If you find that some data operations take too long to complete on your machine, simply use the `sample()` method to transform a subset of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90a38922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360ca62",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Does your model contain any missing values or \"non-predictive\" columns? If so, which adjustments should you take to ensure that your model has good predictive capabilities? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer: There are no missing values in the dataset, as confirmed by checking with df.isnull().sum(). However, we will remove some non-predictive columns, nameOrig, nameDest, and isFlaggedFraud, those data are not features we can use to predict the target variable (isFraud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cfe64ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type              0\n",
       "amount            0\n",
       "nameOrig          0\n",
       "oldbalanceOrg     0\n",
       "newbalanceOrig    0\n",
       "nameDest          0\n",
       "oldbalanceDest    0\n",
       "newbalanceDest    0\n",
       "isFraud           0\n",
       "isFlaggedFraud    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = pd.read_csv(\"../data/bank_transactions.csv\")\n",
    "transactions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adad1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b8dbc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type              0\n",
       "amount            0\n",
       "oldbalanceOrg     0\n",
       "newbalanceOrig    0\n",
       "oldbalanceDest    0\n",
       "newbalanceDest    0\n",
       "isFraud           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.dropna(inplace=True)\n",
    "transactions.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301be5ef",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Do certain transaction types consistently differ in amount or fraud likelihood? If so, how might you transform the type column to make this pattern usable by a machine learning model? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer: Different transaction types show significant differences in fraud rates. TRANSFER and CASH_OUT transactions tend to involve larger amounts, and as shown in the figure, they also have much higher fraud rates compared to other types. The average transaction amount for TRANSFER is around 911,827 and for CASH_OUT it's about 175,585. Their fraud probabilities are 0.76% and 0.19%, while other types have a fraud probability of 0%, meaning fraud almost never occurs in those types. This indicates that fraud is more likely to occur in these large-value, TRANSFER and CASH_OUT transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c722ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "TRANSFER    911827.155179\n",
       "CASH_OUT    175584.659320\n",
       "CASH_IN     168928.914668\n",
       "PAYMENT      13055.592085\n",
       "DEBIT         5445.890813\n",
       "Name: amount, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_amount_mean = transactions.groupby('type')['amount'].mean().sort_values(ascending=False)\n",
    "type_amount_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ed443c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "TRANSFER    0.007647\n",
       "CASH_OUT    0.001870\n",
       "CASH_IN     0.000000\n",
       "DEBIT       0.000000\n",
       "PAYMENT     0.000000\n",
       "Name: isFraud, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.groupby('type')['isFraud'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952403f",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "After exploring your data, you may have noticed that fraudulent transactions are rare compared to non-fraudulent ones. What challenges might this pose when training a machine learning model? What strategies could you use to ensure your model learns meaningful patterns from the minority class? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f1422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17737e9e",
   "metadata": {},
   "source": [
    "## Bonus (optional)\n",
    "\n",
    "Are there interaction effects between variables (e.g., fraud and high amount and transaction type) that aren't captured directly in the dataset? Would it be helpful to manually engineer any new features that reflect these interactions? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81cbfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out newly transformed dataset to your folder\n",
    "transactions.to_csv('../data/filtered_transactions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
